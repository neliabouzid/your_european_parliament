{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization of the completed and ongoing procedure's proposals on my dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM package install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:28:17.273994Z",
     "iopub.status.busy": "2025-12-30T15:28:17.273765Z",
     "iopub.status.idle": "2025-12-30T15:28:48.322878Z",
     "shell.execute_reply": "2025-12-30T15:28:48.322156Z",
     "shell.execute_reply.started": "2025-12-30T15:28:17.273972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-pgkq4oxp/unsloth_25650a19c5d74f87adf95accceb5bdf8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-pgkq4oxp/unsloth_25650a19c5d74f87adf95accceb5bdf8\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8ea5338154859ed25b50366cb1264ed4d933eae3\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2025.12.9-py3-none-any.whl size=382658 sha256=0956f1bb09c1104fbb500c6ff206b17fe4191a5ce2d0470d528a451318145748\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ssbzlqj/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.12.9\n",
      "Collecting xformers<0.0.29\n",
      "  Downloading xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting trl<0.9.0\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Downloading xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers, trl, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.0 trl-0.8.6 xformers-0.0.28.post3\n",
      "Requirement already satisfied: transformers>=4.47.0 in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Collecting transformers>=4.47.0\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers>=0.21.0 in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Collecting tokenizers>=0.21.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers, tokenizers, huggingface-hub\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.1\n",
      "    Uninstalling transformers-4.51.1:\n",
      "      Successfully uninstalled transformers-4.51.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.30.2\n",
      "    Uninstalling huggingface-hub-0.30.2:\n",
      "      Successfully uninstalled huggingface-hub-0.30.2\n",
      "Successfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.29\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install --upgrade --no-deps \"transformers>=4.47.0\" \"tokenizers>=0.21.0\" \"huggingface-hub>=0.26.0,<1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:28:48.324913Z",
     "iopub.status.busy": "2025-12-30T15:28:48.324676Z",
     "iopub.status.idle": "2025-12-30T15:28:48.329297Z",
     "shell.execute_reply": "2025-12-30T15:28:48.328607Z",
     "shell.execute_reply.started": "2025-12-30T15:28:48.324892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# To avoid 'MessageFactory' error on Kaggle\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "# Gives Torch priority to the GPU in comparison to Tenserflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:28:48.330344Z",
     "iopub.status.busy": "2025-12-30T15:28:48.330131Z",
     "iopub.status.idle": "2025-12-30T15:28:55.958082Z",
     "shell.execute_reply": "2025-12-30T15:28:55.957358Z",
     "shell.execute_reply.started": "2025-12-30T15:28:48.330328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import concurrent.futures\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Make sure NLTK data is downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:28:55.959375Z",
     "iopub.status.busy": "2025-12-30T15:28:55.958929Z",
     "iopub.status.idle": "2025-12-30T15:29:54.493354Z",
     "shell.execute_reply": "2025-12-30T15:29:54.492433Z",
     "shell.execute_reply.started": "2025-12-30T15:28:55.959354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a193f678cad436aab30197b276fd6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3111ed6509544f2aa1f4899eb2a38ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4587761940954524a9194fa37ab04264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d481225e0e4643835d9bfa6c10389e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fb1259455a4e5b8b9d24dadc872bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57baf8f61db147e4884be2bd26aab07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767108543.324966      33 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767108543.379229      33 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c9bd191bcc45b4ad06d655ab27b8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0e441e34934ea1a787d2d1eeaa1144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6299251b9d47b1a0d93ce08df10c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c4ad51d44e4efaba5fbf92cd7557f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037fb752ebab49efa6fe353cdff73e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f87fa99b4dd47c0a8483e8e33f72daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622db21cb9264cfca5bdf70510ad19e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"unsloth/phi-4-unsloth-bnb-4bit\"\n",
    "\n",
    "# Quantification configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model with the quantification configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:29:54.494879Z",
     "iopub.status.busy": "2025-12-30T15:29:54.494253Z",
     "iopub.status.idle": "2025-12-30T15:29:54.547711Z",
     "shell.execute_reply": "2025-12-30T15:29:54.547166Z",
     "shell.execute_reply.started": "2025-12-30T15:29:54.494852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To make it work here, first input the sample_cod_completed_2025.csv on Kaggle and name it completed-procedures-2025-27-12\n",
    "# You can swap the dataset with the one that contains the ongoing procedures if you want to specifically summarize them \n",
    "df = pd.read_csv(\"/kaggle/input/completed-procedures-2025-27-12/sample_cod_completed_2025.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:29:54.548656Z",
     "iopub.status.busy": "2025-12-30T15:29:54.548451Z",
     "iopub.status.idle": "2025-12-30T15:29:57.758282Z",
     "shell.execute_reply": "2025-12-30T15:29:57.757307Z",
     "shell.execute_reply.started": "2025-12-30T15:29:54.548639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>reference</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>key_players</th>\n",
       "      <th>key_events</th>\n",
       "      <th>documentation_gateway</th>\n",
       "      <th>transparency</th>\n",
       "      <th>...</th>\n",
       "      <th>Mandatory consultation of other institutions</th>\n",
       "      <th>Other legal basis</th>\n",
       "      <th>legislative_proposal_url</th>\n",
       "      <th>legislative_proposal_text</th>\n",
       "      <th>decisions_url_list</th>\n",
       "      <th>resolution_text_1</th>\n",
       "      <th>position_text_1</th>\n",
       "      <th>text_adopted_others_1</th>\n",
       "      <th>final_act_text</th>\n",
       "      <th>final_act_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>https://oeil.europarl.europa.eu/oeil/en/proced...</td>\n",
       "      <td>2025/0022(COD)</td>\n",
       "      <td>Securities settlement in the EU and central se...</td>\n",
       "      <td>2.50.03 Securities and financial markets, stoc...</td>\n",
       "      <td>{'European Parliament': [{'Committee responsib...</td>\n",
       "      <td>{'Legislative proposal published': {'Date': '1...</td>\n",
       "      <td>[{'Institution': 'European Commission', 'Docum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': 'European Economic and Social Commit...</td>\n",
       "      <td>Rules of Procedure EP 165</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>EUROPEAN COMMISSION  Brussels, 12.2.2025  COM(...</td>\n",
       "      <td>['https://www.europarl.europa.eu/doceo/documen...</td>\n",
       "      <td>European Parliament legislative resolution of ...</td>\n",
       "      <td>Position of the European Parliament adopted at...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULATION (EU) 2025/2075 OF THE EUROPEAN PARL...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>https://oeil.europarl.europa.eu/oeil/en/proced...</td>\n",
       "      <td>2025/0074(COD)</td>\n",
       "      <td>Extension of the timeframe for the establishme...</td>\n",
       "      <td>7.30.30 Action to combat crime, 7.40.04 Judici...</td>\n",
       "      <td>{'European Parliament': [{'Committee responsib...</td>\n",
       "      <td>{'Legislative proposal published': {'Date': '0...</td>\n",
       "      <td>[{'Institution': 'European Commission', 'Docum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rules of Procedure EP 165</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>EUROPEAN COMMISSION  Brussels, 2.4.2025  COM(2...</td>\n",
       "      <td>['https://www.europarl.europa.eu/doceo/documen...</td>\n",
       "      <td>European Parliament legislative resolution of ...</td>\n",
       "      <td>Position of the European Parliament adopted at...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULATION (EU) 2025/2082 OF THE EUROPEAN PARL...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>https://oeil.europarl.europa.eu/oeil/en/proced...</td>\n",
       "      <td>2025/0056(COD)</td>\n",
       "      <td>Common rules for imports: suspension of certai...</td>\n",
       "      <td>6.20.02 Export/import control, trade defence, ...</td>\n",
       "      <td>{'European Parliament': [{'Committee responsib...</td>\n",
       "      <td>{'Legislative proposal published': {'Date': '0...</td>\n",
       "      <td>[{'Institution': 'European Commission', 'Docum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>EUROPEAN COMMISSION  Brussels, 7.3.2025  COM(2...</td>\n",
       "      <td>['https://www.europarl.europa.eu/doceo/documen...</td>\n",
       "      <td>European Parliament legislative resolution of ...</td>\n",
       "      <td>Position of the European Parliament adopted at...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1  Unnamed: 0  \\\n",
       "15           109         109   \n",
       "16           115         115   \n",
       "17           116         116   \n",
       "\n",
       "                                                  url       reference  \\\n",
       "15  https://oeil.europarl.europa.eu/oeil/en/proced...  2025/0022(COD)   \n",
       "16  https://oeil.europarl.europa.eu/oeil/en/proced...  2025/0074(COD)   \n",
       "17  https://oeil.europarl.europa.eu/oeil/en/proced...  2025/0056(COD)   \n",
       "\n",
       "                                                title  \\\n",
       "15  Securities settlement in the EU and central se...   \n",
       "16  Extension of the timeframe for the establishme...   \n",
       "17  Common rules for imports: suspension of certai...   \n",
       "\n",
       "                                             subjects  \\\n",
       "15  2.50.03 Securities and financial markets, stoc...   \n",
       "16  7.30.30 Action to combat crime, 7.40.04 Judici...   \n",
       "17  6.20.02 Export/import control, trade defence, ...   \n",
       "\n",
       "                                          key_players  \\\n",
       "15  {'European Parliament': [{'Committee responsib...   \n",
       "16  {'European Parliament': [{'Committee responsib...   \n",
       "17  {'European Parliament': [{'Committee responsib...   \n",
       "\n",
       "                                           key_events  \\\n",
       "15  {'Legislative proposal published': {'Date': '1...   \n",
       "16  {'Legislative proposal published': {'Date': '0...   \n",
       "17  {'Legislative proposal published': {'Date': '0...   \n",
       "\n",
       "                                documentation_gateway transparency  ...  \\\n",
       "15  [{'Institution': 'European Commission', 'Docum...          NaN  ...   \n",
       "16  [{'Institution': 'European Commission', 'Docum...          NaN  ...   \n",
       "17  [{'Institution': 'European Commission', 'Docum...          NaN  ...   \n",
       "\n",
       "         Mandatory consultation of other institutions  \\\n",
       "15  [{'text': 'European Economic and Social Commit...   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "            Other legal basis  \\\n",
       "15  Rules of Procedure EP 165   \n",
       "16  Rules of Procedure EP 165   \n",
       "17                        NaN   \n",
       "\n",
       "                             legislative_proposal_url  \\\n",
       "15  https://eur-lex.europa.eu/legal-content/EN/TXT...   \n",
       "16  https://eur-lex.europa.eu/legal-content/EN/TXT...   \n",
       "17  https://eur-lex.europa.eu/legal-content/EN/TXT...   \n",
       "\n",
       "                            legislative_proposal_text  \\\n",
       "15  EUROPEAN COMMISSION  Brussels, 12.2.2025  COM(...   \n",
       "16  EUROPEAN COMMISSION  Brussels, 2.4.2025  COM(2...   \n",
       "17  EUROPEAN COMMISSION  Brussels, 7.3.2025  COM(2...   \n",
       "\n",
       "                                   decisions_url_list  \\\n",
       "15  ['https://www.europarl.europa.eu/doceo/documen...   \n",
       "16  ['https://www.europarl.europa.eu/doceo/documen...   \n",
       "17  ['https://www.europarl.europa.eu/doceo/documen...   \n",
       "\n",
       "                                    resolution_text_1  \\\n",
       "15  European Parliament legislative resolution of ...   \n",
       "16  European Parliament legislative resolution of ...   \n",
       "17  European Parliament legislative resolution of ...   \n",
       "\n",
       "                                      position_text_1 text_adopted_others_1  \\\n",
       "15  Position of the European Parliament adopted at...                   NaN   \n",
       "16  Position of the European Parliament adopted at...                   NaN   \n",
       "17  Position of the European Parliament adopted at...                   NaN   \n",
       "\n",
       "                                       final_act_text  \\\n",
       "15  REGULATION (EU) 2025/2075 OF THE EUROPEAN PARL...   \n",
       "16  REGULATION (EU) 2025/2082 OF THE EUROPEAN PARL...   \n",
       "17                                                NaN   \n",
       "\n",
       "                                        final_act_url  \n",
       "15  https://eur-lex.europa.eu/legal-content/EN/TXT...  \n",
       "16  https://eur-lex.europa.eu/legal-content/EN/TXT...  \n",
       "17                                                NaN  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since every summarization takes a lot of time, I run this code sequentially on parts of my dataset.\n",
    "# Here is the code to summarize the 3 last procedures. You can change the parts of the dataset selected if you want to summarize other procedures.\n",
    "df = df[15:] \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions to summarize the procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:29:57.761636Z",
     "iopub.status.busy": "2025-12-30T15:29:57.761320Z",
     "iopub.status.idle": "2025-12-30T15:29:58.656875Z",
     "shell.execute_reply": "2025-12-30T15:29:58.656067Z",
     "shell.execute_reply.started": "2025-12-30T15:29:57.761608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Wrap llm generation into a function\n",
    "def generation(prompt) :\n",
    "  model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a journalist\"},\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "  ]\n",
    "  input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "  outputs = model.generate(input_tensor.to(model.device), max_new_tokens = 300, temperature=0.1, do_sample=True)\n",
    "\n",
    "  result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "  return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:29:58.659546Z",
     "iopub.status.busy": "2025-12-30T15:29:58.657873Z",
     "iopub.status.idle": "2025-12-30T15:29:59.763110Z",
     "shell.execute_reply": "2025-12-30T15:29:59.762219Z",
     "shell.execute_reply.started": "2025-12-30T15:29:58.659498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to prompt phi-4 to summarize the first page of a legislative proposal\n",
    "def paste_global_summary_first_page(first_page, legislative_proposal_title):\n",
    "    prompt = f\"\"\"\n",
    "You are a journalist explaining European legislation to the general public.\n",
    "\n",
    "Below is the first page of the legislative proposal for the procedure \"{legislative_proposal_title}\" that has been submitted to the European Parliament:\n",
    "\n",
    "---\n",
    "{first_page}\n",
    "---\n",
    "\n",
    "Summarize it in one or a few coherent paragraphs.\n",
    "Your summary should:\n",
    "- Explain the adopted proposal's main points.\n",
    "- Remain concise, neutral, and clear. Do not repeat yourself.\n",
    "- Avoid quoting specific articles or amendments.  \n",
    "- Clarify technical or institutional terms in simple language.\n",
    "- Exclude legal boilerplate: Systematically remove standard legal clauses that do not provide specific content to this law.\n",
    "\n",
    "Base your explanation only on the provided text. Do not take sides. Do not defend nor attack the adopted text.\n",
    "Output: a summary, in one or a few coherent paragraphs.\n",
    "\"\"\"\n",
    "\n",
    "    result = generation(prompt)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:29:59.764421Z",
     "iopub.status.busy": "2025-12-30T15:29:59.764163Z",
     "iopub.status.idle": "2025-12-30T15:30:00.531354Z",
     "shell.execute_reply": "2025-12-30T15:30:00.530491Z",
     "shell.execute_reply.started": "2025-12-30T15:29:59.764398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function prompting phi-4 to iteratively summarize the next page of a legislative proposal with its previous summary\n",
    "def paste_global_summary(legislative_proposal_title, previous_summary, new_text, i):\n",
    "    prompt = f\"\"\"\n",
    "You are a journalist explaining European legislation to the general public.\n",
    "\n",
    "Below is the summary written so far, covering the first {i} pages of the proposal for the legislative procedure \"{legislative_proposal_title}\" that has been submitted to the European Parliament:\n",
    "\n",
    "---\n",
    "{previous_summary}\n",
    "---\n",
    "\n",
    "Here is the next page of the proposal:\n",
    "\n",
    "---\n",
    "{new_text}\n",
    "---\n",
    "\n",
    "Update and rewrite the summary so it now covers everything up to this point ({i+1} pages), in one or a few consistent paragraphs.\n",
    "Your summary should:\n",
    "- Explain the proposal's main points.\n",
    "- Remain concise, neutral, and clear. Do not repeat yourself.\n",
    "- Avoid quoting specific articles or amendments.  \n",
    "- Clarify technical or institutional terms in simple language. \n",
    "- Exclude legal boilerplate: Systematically remove standard legal clauses that do not provide specific content to this law.\n",
    "\n",
    "\n",
    "Base your summary only on the provided text. Do not take sides. Do not defend nor attack the adopted text.\n",
    "Output: a summary, in one or a few constitent paragraphs.\n",
    "\"\"\"\n",
    "\n",
    "    result = generation(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:30:00.532494Z",
     "iopub.status.busy": "2025-12-30T15:30:00.532237Z",
     "iopub.status.idle": "2025-12-30T15:30:00.992625Z",
     "shell.execute_reply": "2025-12-30T15:30:00.991842Z",
     "shell.execute_reply.started": "2025-12-30T15:30:00.532471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to split the legislative texts into chunks of 1000 words\n",
    "def text_into_segments(text, max_words=1000):\n",
    "    sentences = sent_tokenize(text)  # Split the text into sentences\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "    current_word_count = 0\n",
    "\n",
    "    # Iterate through each sentence and group them into segments\n",
    "    for sentence in sentences:\n",
    "        sentence_word_count = len(word_tokenize(sentence))\n",
    "        # If adding this sentence exceeds the word limit, start a new segment\n",
    "        if current_word_count + sentence_word_count > max_words:\n",
    "            if current_segment:\n",
    "                segments.append(current_segment.strip())\n",
    "            current_segment = sentence\n",
    "            current_word_count = sentence_word_count\n",
    "        else:\n",
    "            # Otherwise, keep adding sentences to the current segment\n",
    "            current_segment += \" \" + sentence\n",
    "            current_word_count += sentence_word_count\n",
    "\n",
    "    # Append the last segment if any text remains\n",
    "    if current_segment:\n",
    "        segments.append(current_segment.strip())\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:30:00.993831Z",
     "iopub.status.busy": "2025-12-30T15:30:00.993539Z",
     "iopub.status.idle": "2025-12-30T15:30:01.010736Z",
     "shell.execute_reply": "2025-12-30T15:30:01.010052Z",
     "shell.execute_reply.started": "2025-12-30T15:30:00.993806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_and_parse_json(json_str):\n",
    "    # Return None if the string is empty or missing\n",
    "    if not json_str or json_str.strip() == \"\":\n",
    "        return None\n",
    "\n",
    "    # Replace single quotes with double quotes (for valid JSON)\n",
    "    json_str = json_str.replace(\"'\", '\"')\n",
    "\n",
    "    # Fix misplaced brackets like COM(2025]0513 so that it becomes COM(2025)0513\n",
    "    json_str = re.sub(r'\\((\\d+)\\]', r'(\\1)', json_str)\n",
    "\n",
    "    # Convert parentheses into square brackets only when wrapping objects\n",
    "    json_str = re.sub(r'\\(\\s*({.*?})\\s*,\\s*({.*?})\\s*\\)', r'[\\1, \\2]', json_str)\n",
    "\n",
    "    # Add missing quotes around JSON keys\n",
    "    json_str = re.sub(r'(?<={|,)\\s*([a-zA-Z_]\\w*)\\s*(?=:)', r'\"\\1\"', json_str)\n",
    "\n",
    "    # Remove trailing commas before closing braces or brackets\n",
    "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
    "\n",
    "    # Try to parse the cleaned JSON string\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Print error details if parsing fails\n",
    "        print(f\"Parsing error after cleaning: {e}\")\n",
    "        print(f\"Problematic string: {repr(json_str)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:30:01.011727Z",
     "iopub.status.busy": "2025-12-30T15:30:01.011516Z",
     "iopub.status.idle": "2025-12-30T15:30:01.028849Z",
     "shell.execute_reply": "2025-12-30T15:30:01.028192Z",
     "shell.execute_reply.started": "2025-12-30T15:30:01.011710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_first_date_index(df, index):\n",
    "    # Check that the index exists in the DataFrame\n",
    "    if index not in df.index:\n",
    "        return None\n",
    "\n",
    "    # Retrieve the 'documentation_gateway' field for the given index\n",
    "    json_str = df.at[index, 'documentation_gateway']\n",
    "    list_dicts = clean_and_parse_json(json_str)\n",
    "\n",
    "    # If the parsed JSON is empty or invalid, return None\n",
    "    if not list_dicts:\n",
    "        return None\n",
    "\n",
    "    # Extract the first available date from the parsed JSON list\n",
    "    first_date = list_dicts[0].get('Date')\n",
    "    return first_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:30:01.030052Z",
     "iopub.status.busy": "2025-12-30T15:30:01.029670Z",
     "iopub.status.idle": "2025-12-30T15:30:01.043512Z",
     "shell.execute_reply": "2025-12-30T15:30:01.042937Z",
     "shell.execute_reply.started": "2025-12-30T15:30:01.030009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_text(idx, df):\n",
    "    try:\n",
    "        print(f\"Processing procedure {df['reference'][idx]}\")\n",
    "        # Retrieve the raw legislative text\n",
    "        text = str(df['legislative_proposal_text'][idx]) if pd.notna(df['legislative_proposal_text'][idx]) else \"\"\n",
    "\n",
    "        if text.strip() == \"\":\n",
    "            print(f\"Error : 'legislative_proposal_text' is empty for procedure {df['reference'][idx]}\")\n",
    "            return {\"title\": df['title'][idx], \"error\": \"legislative_proposal_text' is empty\"}\n",
    "\n",
    "        # Split the legislative text into segments for processing\n",
    "        texts = text_into_segments(text)\n",
    "        \n",
    "        # Initialize the summary text\n",
    "        summary = \"\"\n",
    "\n",
    "        # Loop through each text segment and build the global summary incrementally\n",
    "        for i, text in enumerate(texts):\n",
    "            if i == 0:\n",
    "                # Generate the first page summary\n",
    "                summary = paste_global_summary_first_page(text, df['title'][idx])\n",
    "            else:\n",
    "                # Continue the summary with additional segments\n",
    "                updated_summary = paste_global_summary(df['title'][idx], summary, text, i)\n",
    "                if not updated_summary:\n",
    "                    print(f\"Summary failed for segment {i} of {df['title'][idx]}\")\n",
    "                else:\n",
    "                    summary = updated_summary\n",
    "        \n",
    "        # Save the generated summary back into the DataFrame\n",
    "        df.at[idx, 'proposal_summary'] = summary\n",
    "        # Save the updated DataFrame to CSV after each summary\n",
    "        df.to_csv('cod_completed_proposal_general_summary_15_17.csv')\n",
    "        print(\"Summary done\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Print any error that occurs during processing\n",
    "        return {\n",
    "            \"title\": df['title'][idx],\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T15:30:01.044453Z",
     "iopub.status.busy": "2025-12-30T15:30:01.044223Z",
     "iopub.status.idle": "2025-12-30T15:40:48.414942Z",
     "shell.execute_reply": "2025-12-30T15:40:48.414275Z",
     "shell.execute_reply.started": "2025-12-30T15:30:01.044437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing procedure 2025/0022(COD)\n",
      "Summary done\n",
      "Processing procedure 2025/0074(COD)\n",
      "Summary done\n",
      "Processing procedure 2025/0056(COD)\n",
      "Summary done\n",
      "Summarization finished\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all of the dataset's rows and process each legislative text\n",
    "for idx in df.index:\n",
    "    result = process_text(idx, df)\n",
    "    del result\n",
    "    # Clear GPU memory after each iteration to prevent memory overflow\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('Summarization finished')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9135746,
     "sourceId": 14310659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9137372,
     "sourceId": 14313420,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
